{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Chapter 6\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the supplementary material of the books \"Online Machine Learning - Eine praxisorientiere EinfÃ¼hrung\",  \n",
    "https://link.springer.com/book/9783658425043 and \"Online Machine Learning - A Practical Guide with Examples in Python\" https://link.springer.com/book/9789819970063\n",
    "The contents are open source and published under the \"BSD 3-Clause License\".\n",
    "This software is provided \"as is\" without warranty of any kind, either express or implied, including but not limited to implied warranties of merchantability and fitness for a particular purpose. The author or authors assume no liability for any damages or liability, whether in contract, tort, or otherwise, arising out of or in connection with the software or the use or other dealings with the software."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Special Requirements for OML Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection using One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import anomaly\n",
    "from river import compose\n",
    "from river import datasets\n",
    "from river import metrics\n",
    "from river import preprocessing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: OneClassSVM\n",
    "\n",
    "* Initialization of the one-class SVM with the following parameters:\n",
    "  * `nu`: an upper bound on the proportion of training errors and a lower bound on the proportion of support vectors. The value can be interpreted as the expected proportion of anomalies. Thus, in our example, 20% anomalies are expected.\n",
    "* For the quantile filter a value of `q=0.995` is chosen. This is the quantile above which an anomaly value is classified as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = anomaly.QuantileFilter(\n",
    "     anomaly.OneClassSVM(nu=0.002),\n",
    "     q=0.995\n",
    " )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CreditCard dataset\n",
    "\n",
    "* The CreditCard dataset contains credit card transactions from European cardholders in September 2013. \n",
    "* This dataset represents transactions that occurred within two days, where we have 492 fraud cases out of 284,807 transactions. \n",
    "* The dataset is unbalanced: \n",
    "  * The positive class (fraud) represents 0.172% of all transactions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric ROC AUC\n",
    "\n",
    "* The ROC AUC (Receiving Operating Characteristic Area Under the Curve) is chosen as the metric because the data set is highly unbalanced and Accuracy, which would otherwise be used, would give a biased result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = metrics.ROCAUC()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and train the model:\n",
    "\n",
    "* Only the first 2500 data sets are used for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in datasets.CreditCard().take(5000):\n",
    "     score = model.score_one(x)\n",
    "     is_anomaly = model.classify(score)\n",
    "     if y > 0:\n",
    "          print(f\"Prediction: {is_anomaly} \\t True value: {y}\")\n",
    "     model = model.learn_one(x)\n",
    "     auc = auc.update(y, is_anomaly)\n",
    "auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: UnbalancedData\n",
    "\n",
    "* In machine learning, imbalanced data is a common occurrence. \n",
    "* This is especially true for OML for tasks such as fraud detection and spam classification.\n",
    "* In these two cases, which are binary classification problems, there are usually many more negative events (`0`) than positive ones (`1`). \n",
    "* As an example, we use the `California Housing` dataset with the parameter `percentile_0 = 95`, i.e. 95% of the entries are `0` (`False`).\n",
    "* We will first use a `Collection.Counter` to count the number of 0s and 1s to check the class split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T17:54:58.523126Z",
     "iopub.status.busy": "2022-09-02T17:54:58.522173Z",
     "iopub.status.idle": "2022-09-02T17:55:03.883554Z",
     "shell.execute_reply": "2022-09-02T17:55:03.884338Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "from river import evaluate\n",
    "from river import preprocessing\n",
    "from river import stream\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "def get_features():\n",
    "    california_housing = fetch_california_housing()\n",
    "    return california_housing.feature_names\n",
    "\n",
    "def get_dataset():\n",
    "    california_housing = fetch_california_housing()\n",
    "    features = california_housing.feature_names\n",
    "    X = pd.DataFrame(california_housing.data, columns=features)\n",
    "    y = pd.Series(california_housing.target)\n",
    "    ## compute the 95% percentile of the target variable y\n",
    "    y_95 = y.quantile(0.75)\n",
    "    print(f\"y_95: {y_95}\")\n",
    "    y = y.apply(lambda x: 1 if x > y_95 else 0)\n",
    "    dataset = stream.iter_pandas(X, y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = collections.Counter(y for _, y in get_dataset())\n",
    "for c, count in counts.items():\n",
    "    print(f'{c}: {count} ({count / sum(counts.values()):.5%})')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data set is unbalanced. \n",
    "* As a starting point, we train a logistic regression model with standard parameters.\n",
    "* We will measure the ROC AUC score, as Accuracy is not a suitable metric for unbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T17:55:03.888893Z",
     "iopub.status.busy": "2022-09-02T17:55:03.888322Z",
     "iopub.status.idle": "2022-09-02T17:55:47.665336Z",
     "shell.execute_reply": "2022-09-02T17:55:47.665705Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from river import compose, preprocessing, linear_model, optim\n",
    "\n",
    "features = get_features()\n",
    "\n",
    "model = (compose.Select(*features) |\n",
    "    preprocessing.StandardScaler() |\n",
    "    linear_model.LogisticRegression()\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "evaluate.progressive_val_score(get_dataset(), model, metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced Data: Importance weighting to handle unbalanced data.\n",
    "\n",
    "* Performance is already acceptable, but can be improved.\n",
    "* The first thing we can do is to increase the proportion of `1` weights by using the `weight_pos` argument of the `log` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T17:55:47.669743Z",
     "iopub.status.busy": "2022-09-02T17:55:47.669158Z",
     "iopub.status.idle": "2022-09-02T17:56:33.200029Z",
     "shell.execute_reply": "2022-09-02T17:56:33.200446Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from river import optim\n",
    "\n",
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    linear_model.LogisticRegression(\n",
    "        loss=optim.losses.Log(weight_pos=5)\n",
    "    )\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "evaluate.progressive_val_score(get_dataset(), model, metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbalanced Data: Focal Loss\n",
    "\n",
    "* [Focal Loss](https://arxiv.org/pdf/1708.02002.pdf) is applied next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T17:56:33.204266Z",
     "iopub.status.busy": "2022-09-02T17:56:33.203696Z",
     "iopub.status.idle": "2022-09-02T17:57:22.573583Z",
     "shell.execute_reply": "2022-09-02T17:57:22.573968Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    linear_model.LogisticRegression(loss=optim.losses.BinaryFocalLoss(2, 1))\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "evaluate.progressive_val_score(get_dataset(), model, metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbalanced Data: Undersampling (of majority class). \n",
    "\n",
    "* Adding weights only works with gradient-based models (including neural networks). \n",
    "* A more generic and potentially more effective approach is to use undersampling and oversampling. \n",
    "* As an example, we undersample the stream so that our logistic regression gets 20% of `1`s and 80% of `0` values. \n",
    "* Under-sampling has the added benefit of requiring fewer training steps and thus reduces overall training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import preprocessing, imblearn, linear_model, metrics, evaluate, stream\n",
    "\n",
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    imblearn.RandomUnderSampler(\n",
    "        classifier=linear_model.LogisticRegression(),\n",
    "        desired_dist={0: .8, 1: .2},\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "evaluate.progressive_val_score(get_dataset(), model, metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `RandomunderSampler` class is a wrapper for classifiers. \n",
    "* This is represented by a rectangle around the logistic regression class when we visualize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced data: Over-sampling (of the minority class).\n",
    "\n",
    "* We can also achieve the same class distribution by increasing the size of the minority class. \n",
    "* This for additional cost for training, since more samples are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T17:58:04.926115Z",
     "iopub.status.busy": "2022-09-02T17:58:04.925477Z",
     "iopub.status.idle": "2022-09-02T17:58:54.450617Z",
     "shell.execute_reply": "2022-09-02T17:58:54.450185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    imblearn.RandomOverSampler(\n",
    "        classifier=linear_model.LogisticRegression(),\n",
    "        desired_dist={0: .8, 1: .2},\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "evaluate.progressive_val_score(get_dataset(), model, metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced data: sampling with the desired sample size.\n",
    "\n",
    "* The disadvantage of both undersampling and oversampling is that we have no control over the amount of data the classifier trains on. \n",
    "* The number of samples is adjusted to achieve the target distribution either by reducing or adding.\n",
    "\n",
    "* However, we can do both at the same time and choose how many samples the classifier gets.\n",
    "* For this purpose, we can use the `RandomSampler` class. \n",
    "* In addition to the desired class distribution, we can specify how much data to train. \n",
    "* The samples are both reduced and multiplied.\n",
    "* This is powerful because you can control both the class distribution and the size of the training data (and thus the training time). \n",
    "* In the following example, we set `sampling_rate=0.1` so that the model trains with 10 percent of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T17:58:54.455262Z",
     "iopub.status.busy": "2022-09-02T17:58:54.454709Z",
     "iopub.status.idle": "2022-09-02T17:59:33.801604Z",
     "shell.execute_reply": "2022-09-02T17:59:33.802320Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    imblearn.RandomSampler(\n",
    "        classifier=linear_model.LogisticRegression(),\n",
    "        desired_dist={0: 0.8, 1: 0.2},\n",
    "        sampling_rate=.01,\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "evaluate.progressive_val_score(get_dataset(), model, metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced Data: A hybrid approach\n",
    "\n",
    "* The methods can also be combined.\n",
    "* Here we combine `RandomUnderSampler` and the parameter `weight_p` from the function `optim.Losses.log`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T17:59:33.806488Z",
     "iopub.status.busy": "2022-09-02T17:59:33.805935Z",
     "iopub.status.idle": "2022-09-02T18:00:14.334367Z",
     "shell.execute_reply": "2022-09-02T18:00:14.334881Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    imblearn.RandomUnderSampler(\n",
    "        classifier=linear_model.LogisticRegression(\n",
    "            loss=optim.losses.Log(weight_pos=5)\n",
    "        ),\n",
    "        desired_dist={0: .8, 1: .2},\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "evaluate.progressive_val_score(get_dataset(), model, metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large number of variables: SelectKBest\n",
    "\n",
    "* The `SelectKBest` method removes all but the $k$ best features from a dataset.\n",
    "* A similarity measure is calculated for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from river import feature_selection\n",
    "from river import stats\n",
    "from river import stream\n",
    "from sklearn import datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "* We create a random regression problem using the `sklearn` method `datasets.make_regression`.\n",
    "* The model uses 10 features. The number of informative features, i.e. the number of features used to build the linear model, is specified by the `n_informative` parameter and is two in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(\n",
    "     n_samples=100,\n",
    "     n_features=10,\n",
    "     n_informative=2,\n",
    "     random_state=1\n",
    " )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the SelectKBest selector.\n",
    "\n",
    "* The parameter `k` of the method `SelectKBest` defines how many features should be kept.\n",
    "* The Pearson correlation is chosen as the similarity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = feature_selection.SelectKBest(\n",
    "     similarity=stats.PearsonCorr(),\n",
    "     k=2\n",
    " )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the selector \n",
    "\n",
    "* After the selector has been trained with the data, the leaderboard can be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xi, yi, in stream.iter_array(X, y):\n",
    "     selector = selector.learn_one(xi, yi)\n",
    "pprint(selector.leaderboard)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AnschlieÃend kÃ¶nnen die `k` (hier 2) besten Merkmale ausgewÃ¤hlt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.transform_one(xi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large number of variables: Poisson Inclusion\n",
    "\n",
    "* The `PoissonInclusion` procedure randomly decides whether a new feature is added. \n",
    "* When a new feature is encountered, it is selected with probability $p$. \n",
    "* The frequency with which a feature must be seen before it is added to the model, \n",
    "follows a geometric distribution with expected value $1/p$.\n",
    "* This feature selection method should be used when there are a very large number of features, only a few of which are \n",
    "are useful, i.e., in situations with \"sparse features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from river import feature_selection\n",
    "from river import stream\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Set: TrumpApproval \n",
    "\n",
    "* FiveThirtyEight is a U.S. news website (editor-in-chief is Nate Silver) focused on statistics and data journalism. \n",
    "* The TrumpApproval dataset is based on data used by FiveThirtyEight to analyze Donald Trump's approval ratings. \n",
    "  * It contains five features that are approval ratings collected by five polling organizations. \n",
    "* The target variable is the approval rating from FiveThirtyEight's model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = iter(datasets.TrumpApproval())\n",
    "feature_names = next(dataset)[0].keys()\n",
    "feature_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now `PoissonInclusion` is executed incrementally for the dataset.\n",
    "* When all five features have been selected, the procedure is terminated and the number of steps is indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = feature_selection.PoissonInclusion(p=0.1, seed=1)\n",
    "n = 0\n",
    "while True:\n",
    "    x, y = next(dataset)\n",
    "    xt = selector.transform_one(x)\n",
    "    if xt.keys() == feature_names:\n",
    "        break\n",
    "    n += 1\n",
    "\n",
    "print(f\"Anzahl der Schritte, bis alle Merkmale gefunden wurden: {n}\")\n",
    "feature_names\n",
    "xt.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large number of variables: VarianceThreshold\n",
    "\n",
    "* This method removes features with low variance.\n",
    "* The minimum required variance of a feature is controlled by the `threshold` parameter:\n",
    "  * The default `threshold` value is `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import feature_selection\n",
    "from river import stream\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple data set\n",
    "\n",
    "* We use a simple data set with four features. \n",
    "* The first and third features have only identical values and thus zero variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "    [0, 2, 0, 3],\n",
    "    [0, 1, 4, 3],\n",
    "    [0, 1, 1, 3]\n",
    "]\n",
    "\n",
    "selector = feature_selection.VarianceThreshold(threshold=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the end, only the second and third components are considered because their variance is greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, _ in stream.iter_array(X):\n",
    "    print(selector.learn_one(x).transform_one(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability and explainability\n",
    "\n",
    "## Example: Interpretability\n",
    "\n",
    "* This example shows how to perform time series modeling using radial basis functions (RBFs).\n",
    "  * The quadratic-exponential kernel is used for local modeling.\n",
    "  * Thus, the influence of individual time components (days, weeks, ...) on the target variable can be estimated.\n",
    "  * Thus, the explanatory power of the model is improved.\n",
    "* The representation in this example is experimental and serves to show the concept (\"prrof-of-concept\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "from river import evaluate\n",
    "from river import preprocessing\n",
    "from river import optim\n",
    "from river import stream\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from datetime import datetime,timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten: California Housing zur Regreesion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we load the California Housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "california_housing = fetch_california_housing()\n",
    "features = california_housing.feature_names\n",
    "x = pd.DataFrame(california_housing.data, columns=features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since the original California Housing data does not have a time stamp, we are adding this.\n",
    "  * The start date is January 1, 1975.\n",
    "* For control purposes, we output the first three time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(1975, 1, 1)\n",
    "tag ={}\n",
    "for day in range(0, x.index.max()+1): \n",
    "   tag[day] = start+timedelta(days=day)\n",
    "   \n",
    "for k, v in tag.items():\n",
    "    if  k<3:\n",
    "        print(k, v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first features now have the following shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['moment'] = pd.Series(tag)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = pd.Series(np.ravel(dataset.data.target))\n",
    "y = pd.Series(california_housing.target)\n",
    "dataset = stream.iter_pandas(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The method `toordinal` returns the proleptic Gregorian ordinal number (the 1st of January of the year 1 is the day 1 and all dates before this day are then called proleptically dated. They have negative year numbers respectively are described with \"before the reckoning of time\" and similar terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ordinal_date(x):\n",
    "    return {'ordinal_date': x['moment'].toordinal()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Linear Regression\n",
    "\n",
    "* We define a pipeline that uses the ordinal datum and transforms the data to mean zero and standard deviation one using `StandardScaler`.\n",
    "* A linear regression model is created using this preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compose.Pipeline(\n",
    "    ('ordinal_date', compose.FuncTransformer(get_ordinal_date)),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The method `evaluate_plot_model`.\n",
    "\n",
    "* We define a method to visualize the original data as well as the data predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "from river import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_plot_model(model): \n",
    "    metric = utils.Rolling(metrics.MAE(), 7)\n",
    "    dates = []\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    i = 0 # ZCounter: days per year\n",
    "    for x, y in dataset:\n",
    "\n",
    "        # Obtain the prior prediction and update the model in one go\n",
    "        y_pred = model.predict_one(x)\n",
    "        model.learn_one(x, y)\n",
    "\n",
    "        # Update the error metric\n",
    "        metric.update(y, y_pred)\n",
    "\n",
    "        # Store the true value and the prediction\n",
    "        dates.append(x['moment'])\n",
    "        y_trues.append(y)\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "        i = i +1\n",
    "        if i > 365:\n",
    "            break\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.grid(alpha=0.75)\n",
    "    ax.plot(dates, y_trues, lw=2, color='#2ecc71', alpha=0.8, label='Ground truth')\n",
    "    ax.plot(dates, y_preds, lw=2, color='#e74c3c', alpha=0.8, label='Prediction')\n",
    "    ax.legend()\n",
    "    ax.set_title(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_plot_model(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd attempt with explicit specification of `intercept_lr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stats\n",
    "\n",
    "dataset = stream.iter_pandas(x, y)\n",
    "model = compose.Pipeline(\n",
    "    ('ordinal_date', compose.FuncTransformer(get_ordinal_date)),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression(intercept_lr=.005)),\n",
    ")\n",
    "\n",
    "#model = preprocessing.TargetStandardScaler(regressor=model)\n",
    "\n",
    "evaluate_plot_model(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinzunahme der Information Ã¼ber den Monat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "dataset = stream.iter_pandas(x, y)\n",
    "def get_month(x):\n",
    "    return {\n",
    "        calendar.month_name[month]: month == x['moment'].month\n",
    "        for month in range(1, 13)\n",
    "    }\n",
    "\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('ordinal_date', compose.FuncTransformer(get_ordinal_date)),\n",
    "        ('month', compose.FuncTransformer(get_month)),\n",
    "    )),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression(intercept_lr=.1))\n",
    ")\n",
    "evaluate_plot_model(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Illustration of the contributions (effects) of the individual months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['lin_reg'].weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import optim\n",
    "\n",
    "dataset = stream.iter_pandas(x, y)\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('ordinal_date', compose.FuncTransformer(get_ordinal_date)),\n",
    "        ('month', compose.FuncTransformer(get_month)),\n",
    "    )),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression(\n",
    "        intercept_lr=0,\n",
    "        optimizer=optim.SGD(0.03)\n",
    "    ))\n",
    ")\n",
    "evaluate_plot_model(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of a kernel function\n",
    "\n",
    "* We do not use the simple distances, but weight them by means of a kernel function:\n",
    "  * We use the quadratic-exponential kernel, so `math.exp(-((x['moment'].day % 2 - day) ** 2 ) / sigma)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "dataset = stream.iter_pandas(x, y)\n",
    "sigma = 1e-6\n",
    "\n",
    "def get_month_distances(x):\n",
    "    #print(x['moment'].day % 7)\n",
    "    return {\n",
    "        calendar.day_name[day]: math.exp(-((x['moment'].day % 2 - day) ** 2 ) / sigma)\n",
    "        for day in range(0,2)\n",
    "    }\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('ordinal_date', compose.FuncTransformer(get_ordinal_date)),\n",
    "        ('month_distances', compose.FuncTransformer(get_month_distances)),\n",
    "    )),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression(\n",
    "        intercept_lr=0,\n",
    "        optimizer=optim.SGD(0.03)\n",
    "    ))\n",
    ")\n",
    "evaluate_plot_model(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Considerations:\n",
    "  \n",
    "* The California Housing dataset was used only to demonstrate technical implementation.\n",
    "  * It would make more sense to use a dataset that implicitly includes seasonal effects (e.g., airpassenger).\n",
    "* A transformation of the target, e.g. via `model = preprocessing.TargetStandardScaler(regressor=model)`, is sometimes helpful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StatImputer\n",
    "\n",
    "* A simple example demonstrating the use of the class 'StatImputer'.\n",
    "* Using a simple data set, mean imputation is demonstrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import preprocessing\n",
    "from river import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "* The fourth measured value is missing (is 'None') and is to be replaced by the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "    {'temperature': 1},\n",
    "    {'temperature': 8},\n",
    "    {'temperature': 3},\n",
    "    {'temperature': None},\n",
    "    {'temperature': 4}\n",
    "]\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-value imputation\n",
    "\n",
    "* After imputation, the fourth measured value is replaced by the mean value\n",
    "  * $(1+8+3)/3$\n",
    "* has been replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = preprocessing.StatImputer(('temperature', stats.Mean()))\n",
    "\n",
    "for x in X:\n",
    "    imp = imp.learn_one(x)\n",
    "    #imp = imp.predict_one(x)\n",
    "    print(imp.transform_one(x))\n",
    "{'temperature': 1}\n",
    "{'temperature': 8}\n",
    "{'temperature': 3}\n",
    "{'temperature': 4.0}\n",
    "{'temperature': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('spotCondaEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81c77de872def749acd68d9955e19f0df6803301f4c1f66c3444af66334112ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
