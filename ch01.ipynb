{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Chapter 1\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the supplementary material of the books \"Online Machine Learning - Eine praxisorientiere EinfÃ¼hrung\",  \n",
    "https://link.springer.com/book/9783658425043 and \"Online Machine Learning - A Practical Guide with Examples in Python\" https://link.springer.com/book/9789819970063\n",
    "The contents are open source and published under the \"BSD 3-Clause License\".\n",
    "This software is provided \"as is\" without warranty of any kind, either express or implied, including but not limited to implied warranties of merchantability and fitness for a particular purpose. The author or authors assume no liability for any damages or liability, whether in contract, tort, or otherwise, arising out of or in connection with the software or the use or other dealings with the software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction: From Batch to Online Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Linear regression is a powerful tool for modeling linear relationships in data.\n",
    "It does this by finding the value of $\\beta$ that minimizes a given cost function **L**. \n",
    "For this, there is a closed-form solution (normal equation) given by:\n",
    "\n",
    "$$\\hat\\beta = (X^{T}X)^{-1}X^{T}y$$\n",
    "\n",
    "In this equation, $\\hat\\beta$ is the value of $\\beta$ that minimizes the cost function, **y** is the vector of objective values.\n",
    "\n",
    "If there is an exact solution, why don't we use it to calculate the values of $\\beta$?\n",
    "\n",
    "The answer is that as the size of the data sets increases, the computational complexity increases. To find a solution quickly for large data sets, gradient descent algorithms can be used. These solve the problem as an iterative process. The simplest approach is batch gradient descent, which uses the full data set at all times. However, this can be more time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "We consider the linear relationship between two variables $x$ and $y$:\n",
    "\n",
    "$$y = 6x + 2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fun_lin(X, sigma):\n",
    "        y = 2.0 + (6.0 * X) # ground truth, to be rediscovered b_0 = 2, b_1 = 6\n",
    "        if sigma != 0:\n",
    "            noise_y = np.array([], dtype=float)\n",
    "            for i in y:\n",
    "                noise_y = np.append(\n",
    "                    noise_y, i + np.random.normal(loc=0, scale=sigma, size=1)\n",
    "                )\n",
    "            return noise_y\n",
    "        else:\n",
    "            return y\n",
    "x = np.linspace(start=1, stop=10)\n",
    "y = fun_lin(x, sigma = 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([x,y])\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Data')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The error function\n",
    "\n",
    "The error function calculates the error between the predicted values and the actual values. We use the quadratic error function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssr_gradient(x, y, b):\n",
    "    res = b[0] + b[1] * x - y\n",
    "    return res.mean(), (res * x).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(gradient, x, y, start, learn_rate=0.0001 , n_iter=50, tolerance = 1e-6):\n",
    "    \"\"\"Performs gradient descent optimization for a function whose gradient is known.\n",
    "\n",
    "    Args:\n",
    "        gradient (callable): The gradient of the function to be minimized.\n",
    "        start (float): The starting point for the optimization.\n",
    "        learn_rate (float): The learning rate for the gradient descent step.\n",
    "        n_iter (int): The number of iterations to perform.\n",
    "        tolerance (float): The stopping criteria.\n",
    "\n",
    "    Returns:\n",
    "        float: The value of x that optimizes the function.\n",
    "\n",
    "    Examples:\n",
    "        >>> gradient_descent(ssr_gradient,x, y, start=[2.5, 6.5], learn_rate=0.00001, n_iter=1000)\n",
    "            b: 2.474687807007182, m:6.332722751822451\n",
    "    \"\"\"\n",
    "    vector = start\n",
    "    for _ in range(n_iter):\n",
    "        diff = -learn_rate * np.array(gradient(x,y, vector))\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        vector += diff\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, m = gradient_descent(ssr_gradient, x, y, start=[2.5, 6.5], learn_rate=0.00001, n_iter=1000)\n",
    "print(f\"b: {b}, m:{m}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the line of the best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot dataset\n",
    "plt.scatter(x, y)\n",
    "#Predict y values\n",
    "pred = m * x + b\n",
    "#Plot predictions as line of best fit\n",
    "plt.plot(x, pred, c='r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Line of the best fit')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD).\n",
    "\n",
    "SGD uses only a subset of the training data in each step, which speeds up the calculation. On the other hand, due to its stochastic nature. This algorithm is much less regular than Batch Gradient Descent. Instead of decreasing smoothly until it reaches the minimum, the cost function bounces up and down, decreasing only on average. Over time, it will end up very close to the minimum, but once it gets there, it will bounce back and forth and never settle down. Once the algorithm stops, the final parameter values will be good, but not optimal.\n",
    "\n",
    "### How does a gradient descent algorithm work?\n",
    "\n",
    "1. choose a random starting point $x_0$\n",
    "2. $x_1$ = $x_0 - r[(df/dx) \\text{ of } x_0]$\n",
    "3. $x_2$ = $x_1 - r[(df/dx) \\text{ from } x_1]$\n",
    "4. continue until convergence.\n",
    "\n",
    "Where $r$ is the learning rate and $df/dx$ is the gradient function to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Implementation\n",
    "\n",
    "We will implement the Stochastic Gradient Descent (SGD) function and test it on our own synthetic data.\n",
    "We will cover the following steps:\n",
    "\n",
    "1. create our own synthetic data, with a sample of at least 50 entities. With one dependent variable and one independent variable.\n",
    "2. visualize our data\n",
    "3. define the gradient function\n",
    "4. define the SGD function. The following parameters are needed:\n",
    "     - Gradient: Defines the gradient function\n",
    "     - x: The values of the independent variables\n",
    "     - y: The values of the dependent variables\n",
    "     - start: a vector that serves as a starting point for our search\n",
    "     - learn_rate: is the learning rate that controls the size of the vector update\n",
    "     - n_iter: number of iterations\n",
    "     - tolerance: termination criteria\n",
    "     - k: Number of point pairs used in each iteration.\n",
    "5. use our self-build function with the following parameters\n",
    "     - start: [2.5, 6.5]\n",
    "     - learn_rate: 0.0001\n",
    "     - n_iter: 10000\n",
    "     - tolerance = 1e-6\n",
    "     - k = 30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the SGD Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(gradient, x, y, start, learn_rate=0.1 , n_iter=50, tolerance = 1e-6, k = 30):\n",
    "    \"\"\" Stochastic gradient function to find efficient values for intercept and slope\n",
    "\n",
    "    Args:\n",
    "        gradient: The gradient function defined above.\n",
    "        x (float): Input value independent variable (1-dim)\n",
    "        y (float): input value dependent variable (1-dim)\n",
    "        vecctor (float): np.array containing initial values for intercept and slope\n",
    "        learn_rate (float): Learn rate\n",
    "        n_iter (int): number of iterations\n",
    "        tolerance (float): the value at which the process is terminated if the slope is smaller\n",
    "        k (int): number of point pairs used in each iteration\n",
    "\n",
    "    Returns:\n",
    "        values for the intercept (float) and the slope (float)\n",
    "\n",
    "    \"\"\"\n",
    "    vector = start\n",
    "    iteration = 1\n",
    "    if k > x.shape[0]:\n",
    "            k = x.shape[0]\n",
    "    while iteration <= n_iter:\n",
    "        indices = np.random.choice(x.shape[0], k, replace=False)\n",
    "        x_sample = x[indices]\n",
    "        y_sample = y[indices]\n",
    "        diff = -learn_rate * np.array(gradient(x_sample,y_sample, vector))\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        vector += diff\n",
    "        iteration +=1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the SGD with the given parameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b, m = SGD(ssr_gradient, x, y, start=[2.5, 6.5], learn_rate=0.0001, n_iter=10000, k = 3)\n",
    "b, m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of the SGD-Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "pred = m * x + b\n",
    "plt.plot(x, pred, c='r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Data and line of best fit')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation why SGD is better suited for large data sets.\n",
    "\n",
    "Unlike batch gradient descent, stochastic gradient descent optimizes only a subsample. Since less data needs to be adjusted in each iteration, the processing time of the algorithm is significantly accelerated.\n",
    "\n",
    "## Explanation of why SGD does not give you the optimal solution.\n",
    "\n",
    "Instead of decreasing smoothly until it reaches the minimum, the cost function bounces up and down, decreasing only on average. When it crosses, it will be very close to the minimum, but once it gets there, it will continue to bounce back and forth and never settle down. Once the algorithm stops, the final parameter values are good, but not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotCondaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
