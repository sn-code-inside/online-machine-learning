{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Chapter 8\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the supplementary material of the books \"Online Machine Learning - Eine praxisorientiere Einführung\",  \n",
    "https://link.springer.com/book/9783658425043 and \"Online Machine Learning - A Practical Guide with Examples in Python\" https://link.springer.com/book/9789819970063\n",
    "The contents are open source and published under the \"BSD 3-Clause License\".\n",
    "This software is provided \"as is\" without warranty of any kind, either express or implied, including but not limited to implied warranties of merchantability and fitness for a particular purpose. The author or authors assume no liability for any damages or liability, whether in contract, tort, or otherwise, arising out of or in connection with the software or the use or other dealings with the software."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Short Introduction to River"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Learning with `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### California Housing\n",
    "\n",
    "* As a simple example of batch learning, suppose we want to learn to predict whether or not the price of a house in California is above the median house price."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning:\n",
    "There are at least three different variants of the `California Housing` dataset:\n",
    "1. the `original` dataset (statLib)\n",
    "2. the `sklearn` dataset\n",
    "3. the `kaggle` dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We first use the `sklearn` dataset, which can be called as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_75: 2.6472499999999997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "cal_housing = fetch_california_housing()\n",
    "features = cal_housing.feature_names\n",
    "X = pd.DataFrame(cal_housing.data, columns=features)\n",
    "y = pd.Series(cal_housing.target)\n",
    "## compute the 95% percentile of the target variable y\n",
    "y_75 = y.quantile(0.75)\n",
    "print(f\"y_75: {y_75}\")\n",
    "y = y.apply(lambda x: 1 if x > y_75 else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our goal is to assign a set of characteristics to a binary decision using logistic regression.\n",
    "* Like many other models based on numerical weights, logistic regression is sensitive to feature scaling. Rescaling the data so that each characteristic has a mean of 0 and a variance of 1 is generally considered best practice. We can apply rescaling and fit the logistic regression sequentially in an elegant way using a pipeline.\n",
    "* To measure the performance of the model, we evaluate the average accuracy using 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.871 (± 0.005)\n"
     ]
    }
   ],
   "source": [
    "# Define the steps of the model\n",
    "model = pipeline.Pipeline([\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LogisticRegression(solver='lbfgs'))\n",
    "])\n",
    "\n",
    "# Define a deterministic cross-validation procedure\n",
    "cv = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Compute the MSE values\n",
    "scorer = metrics.make_scorer(metrics.accuracy_score)\n",
    "scores = model_selection.cross_val_score(model, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Display the average score and it's standard deviation\n",
    "print(f'Accuracy: {scores.mean():.3f} (± {scores.std():.3f})')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `river` package\n",
    "\n",
    "* `river` works \"by design\" with dictionaries (Python dictionaries), so called `dicts`. \n",
    "* The `river` programmers believe that it is more convenient to program with `dicts` than with `numpy.ndarrays`, at least when it comes to single observations:\n",
    "  * `dicts` have the added advantage that each function can be accessed by name rather than by position.\n",
    "* Conveniently, river's `stream` module has an `iter_sklearn_dataset` method that we can use to convert `sklearn` data to `river` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_95: 2.6472499999999997\n",
      "{'MedInc': 8.3252, 'HouseAge': 41.0, 'AveRooms': 6.984126984126984, 'AveBedrms': 1.0238095238095237, 'Population': 322.0, 'AveOccup': 2.5555555555555554, 'Latitude': 37.88, 'Longitude': -122.23} 1\n"
     ]
    }
   ],
   "source": [
    "from river import stream\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "def get_features():\n",
    "    california_housing = fetch_california_housing()\n",
    "    return california_housing.feature_names\n",
    "\n",
    "def get_dataset():\n",
    "    california_housing = fetch_california_housing()\n",
    "    features = california_housing.feature_names\n",
    "    X = pd.DataFrame(california_housing.data, columns=features)\n",
    "    y = pd.Series(california_housing.target)\n",
    "    ## compute the 95% percentile of the target variable y\n",
    "    y_75 = y.quantile(0.75)\n",
    "    print(f\"y_95: {y_75}\")\n",
    "    y = y.apply(lambda x: 1 if x > y_75 else 0)\n",
    "    dataset = stream.iter_pandas(X, y)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset=get_dataset()\n",
    "\n",
    "for xi, yi in dataset:    \n",
    "    print(xi, yi)\n",
    "    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since the algorithms operate on a stream of data, many calculations cannot be performed as in classic batch mode.\n",
    "  * For example, suppose we want to scale the data so that it has mean 0 and variance 1.  To do this in the BML, we simply need to subtract the mean of each feature from each value and then divide the result by the standard deviation of the feature.  The problem is that we cannot possibly know the values of the mean and standard deviation before we have actually gone through all the data. \n",
    "* One approach would be to make a first pass over the data to calculate the required values, and then scale the values during a second pass. \n",
    "* The problem is that this does not meet our requirement that the data only be used once.\n",
    "\n",
    "* The way we do feature scaling in `river` involves calculating run statistics (runtime statistics). \n",
    "* The idea is that we use a data structure that estimates the mean and updates itself when given a value. The same is true for variance (and standard deviation). \n",
    "* For example, if $\\mu_t$ represents the mean and $n_t$ represents the number of samples at time $t$, then the mean can be updated as follows:\n",
    "$$n_{t+1} = n_t +1  \\\\\n",
    "\\mu_{t+1} = \\mu_t + (x -\\mu_t)/n_{t+1}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the running mean (the variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running mean: 1425.477\n",
      "Running variance: 1282408.322\n"
     ]
    }
   ],
   "source": [
    "n, mean, sum_of_squares, variance = 0, 0, 0, 0\n",
    "\n",
    "def get_reg_dataset():\n",
    "    california_housing = fetch_california_housing()\n",
    "    features = california_housing.feature_names\n",
    "    X = pd.DataFrame(california_housing.data, columns=features)\n",
    "    y = pd.Series(california_housing.target)\n",
    "    dataset = stream.iter_pandas(X, y)\n",
    "    return dataset\n",
    "\n",
    "for xi, yi in get_reg_dataset():\n",
    "    n += 1\n",
    "    old_mean = mean\n",
    "    mean += (xi['Population'] - mean) / n\n",
    "    sum_of_squares += (xi['Population'] - old_mean) * (xi['Population'] - mean)\n",
    "    variance = sum_of_squares / n\n",
    "\n",
    "print(f'Running mean: {mean:.3f}')\n",
    "print(f'Running variance: {variance:.3f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For comparison, we calculate the classical (batch) values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean: 1425.477\n",
      "True variance: 1282470.457\n"
     ]
    }
   ],
   "source": [
    "california_housing = fetch_california_housing()\n",
    "features = california_housing.feature_names\n",
    "X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n",
    "print(f'True mean: {X[\"Population\"].mean():.3f}')\n",
    "print(f'True variance: {X[\"Population\"].var():.3f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The results are similar: \n",
    "* The running statistics for the first observations are not very accurate. However, in general this does not matter too much. \n",
    "* The statistics can thus be updated when a new sample arrives. They can be used to scale the characteristics.\n",
    "  * In `river` methods from the `StandardScaler` class are available for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "for xi, yi in get_reg_dataset():\n",
    "    scaler.learn_one(xi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the following, we will implement a linear online classification task using logistic regression. \n",
    "* Since not all data is available at once, we have to perform the so-called stochastic gradient descent (SGD).\n",
    "  * SGD is often used to train neural networks. \n",
    "  * The idea is that at each step we compute the loss between the target prediction and the truth. \n",
    "  * We then calculate the gradient, which is simply a set of derivatives with respect to each weight from the linear regression.\n",
    "  * Once we have obtained the gradient, we can update the weights by moving them in the opposite direction of the gradient. \n",
    "  * The amount by which the weights are moved depends on a learning rate.  Different optimizers have different ways of managing the weight update, and some handle the learning rate implicitly.\n",
    "  * Online linear regression can be done in `river` using the LinearRegression class from the `linear_model` module. \n",
    "* We simply use SGD with the SGD optimizer from the `optim` module. During the training we measure the squared error between the observed and the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_95: 2.6472499999999997\n",
      "Accuracy: 0.912\n"
     ]
    }
   ],
   "source": [
    "from river import linear_model\n",
    "from river import optim\n",
    "import statistics\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "optimizer = optim.SGD(lr=0.01)\n",
    "log_reg = linear_model.LogisticRegression(optimizer)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for xi, yi in get_dataset():\n",
    "    \n",
    "    # Before river 0.21.0:\n",
    "    # Scale the features\n",
    "    # xi_scaled = scaler.learn_one(xi).transform_one(xi)\n",
    "\n",
    "    # After 0.21.0:\n",
    "    scaler.learn_one(xi)\n",
    "    xi_scaled = scaler.transform_one(xi)\n",
    "\n",
    "\n",
    "    # Test the current model on the new \"unobserved\" sample\n",
    "    yi_pred = log_reg.predict_proba_one(xi_scaled)\n",
    "    # Train the model with the new sample\n",
    "    log_reg.learn_one(xi_scaled, yi)\n",
    "\n",
    "    # Store the truth and the prediction\n",
    "    y_true.append(yi)\n",
    "    y_pred.append(yi_pred[True])\n",
    "\n",
    "print(f'Accuracy: {metrics.roc_auc_score(y_true, y_pred):.3f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy is better than that obtained from scikit-learn logistic regression cross-validation. \n",
    "* However, to make things truly comparable, it would be nice to compare using the same cross-validation procedure. \n",
    "* `river` has a compat module that contains utilities to make river compatible with other Python libraries. \n",
    "* Since we are doing regression, we will use `SKLRegressorWrapper`. \n",
    "* We will also use pipeline to encapsulate the logic of the StandardScaler and LogisticRegression in a single object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following steps are implemented:\n",
    "    * Define a river Pipeline, exactly like done earlier for sklearn \n",
    "    * Make the Pipeline compatible with sklearn\n",
    "    * Compute the CV scores using the same CV scheme and the same scoring\n",
    "    * Display the average score and it's standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.842 (± 0.006)\n"
     ]
    }
   ],
   "source": [
    "from river import compat\n",
    "from river import compose\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('log_reg', linear_model.LogisticRegression())\n",
    ")\n",
    "model = compat.convert_river_to_sklearn(model)\n",
    "scores = model_selection.cross_val_score(model, X, y, scoring=scorer, cv=cv)\n",
    "print(f'Accuracy: {scores.mean():.3f} (± {scores.std():.3f})')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy is lower this time, which we would expect. \n",
    "* In fact, online learning is not as accurate as batch learning. \n",
    "* However, it all depends on what you are interested in:\n",
    "  * If you are only interested in predicting the next observation, the OML algorithm would be better. \n",
    "* For this reason, it is somewhat difficult to compare the two approaches: \n",
    "  * They are both suitable for different scenarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer in river"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We create a simple example data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'c1': 'u', 'c2': 'd'},\n",
      " {'c1': 'a', 'c2': 'x'},\n",
      " {'c1': 'i', 'c2': 'h'},\n",
      " {'c1': 'h', 'c2': 'e'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import random\n",
    "import string\n",
    "\n",
    "random.seed(42)\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "X = [\n",
    "    {\n",
    "        'c1': random.choice(alphabet),\n",
    "        'c2': random.choice(alphabet),\n",
    "    }\n",
    "    for _ in range(4)\n",
    "]\n",
    "pprint(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply `one-hot-encoding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c1_u': 1, 'c2_d': 1}\n",
      "{'c1_a': 1, 'c1_u': 0, 'c2_d': 0, 'c2_x': 1}\n",
      "{'c1_a': 0, 'c1_i': 1, 'c1_u': 0, 'c2_d': 0, 'c2_h': 1, 'c2_x': 0}\n",
      "{'c1_a': 0,\n",
      " 'c1_h': 1,\n",
      " 'c1_i': 0,\n",
      " 'c1_u': 0,\n",
      " 'c2_d': 0,\n",
      " 'c2_e': 1,\n",
      " 'c2_h': 0,\n",
      " 'c2_x': 0}\n"
     ]
    }
   ],
   "source": [
    "from river import preprocessing\n",
    "\n",
    "oh = preprocessing.OneHotEncoder()\n",
    "for x in X:\n",
    "    oh.learn_one(x)\n",
    "    pprint(oh.transform_one(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A subset of the features can be one-hot coded using `compose.Select`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c1_u': 1}\n",
      "{'c1_a': 1, 'c1_u': 0}\n",
      "{'c1_a': 0, 'c1_i': 1, 'c1_u': 0}\n",
      "{'c1_a': 0, 'c1_h': 1, 'c1_i': 0, 'c1_u': 0}\n"
     ]
    }
   ],
   "source": [
    "from river import compose\n",
    "\n",
    "pp = compose.Select('c1') | preprocessing.OneHotEncoder()\n",
    "\n",
    "for x in X:\n",
    "    pp.learn_one(x)\n",
    "    pprint(pp.transform_one(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('spotCondaEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81c77de872def749acd68d9955e19f0df6803301f4c1f66c3444af66334112ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
