{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Chapter 2\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the supplementary material of the books \"Online Machine Learning - Eine praxisorientiere Einführung\",  \n",
    "https://link.springer.com/book/9783658425043 and \"Online Machine Learning - A Practical Guide with Examples in Python\" https://link.springer.com/book/9789819970063\n",
    "The contents are open source and published under the \"BSD 3-Clause License\".\n",
    "This software is provided \"as is\" without warranty of any kind, either express or implied, including but not limited to implied warranties of merchantability and fitness for a particular purpose. The author or authors assume no liability for any damages or liability, whether in contract, tort, or otherwise, arising out of or in connection with the software or the use or other dealings with the software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Supervised Learning: Classification and Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear online regression with River\n",
    "\n",
    "* Linear online regression can be performed in `river` with the `LinearRegression` class \n",
    "from the `linear_model` module. \n",
    "* In the following example, the MAE (mean absolute error) error is measured during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Daten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we create a dataset for classification using the `sklearn.datasets` class `make_classification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a classification dataset with sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=2, n_redundant=2, random_state=42)\n",
    "\n",
    "## Convert X to a pandas dataframe\n",
    "import pandas as pd\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Selection and Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Mean Absolute Error (MAE) is selected as the metric.\n",
    "* In addition, the scaling of the data is specified:\n",
    "  * By selecting the `StandardScalers` class, the data is scaled to have zero mean and one variance.\n",
    "  * Internally, a running mean and running variance are determined. This scaling differs slightly from the scaling of the data in the batch because the exact means and variances are not known in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "from river import preprocessing\n",
    "\n",
    "metric = metrics.MAE()\n",
    "scaler = preprocessing.StandardScaler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of the model\n",
    "\n",
    "* In the next step the model (here the `LinearRegression`) is selected.\n",
    "* For the sequential online optimization of the model coefficients, the stochastic gradient descent (`SGD`) is selected.\n",
    "  * The learning rate for `SGD` is set to 0.01. \n",
    "  * The setting of a suitable learning rate is crucial for the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import optim\n",
    "from river import linear_model\n",
    "\n",
    "optimizer = optim.SGD(lr=0.01)\n",
    "lin_reg = linear_model.LinearRegression(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following example is based on [https://riverml.xyz/latest/recipes/pipelines/](https://riverml.xyz/latest/recipes/pipelines/).\n",
    "* The compose.Pipeline contains all the logic for building and applying pipelines.\n",
    "* A pipeline is essentially a list of estimators that are applied in sequence.\n",
    "* The only requirement is that the first n - 1 steps be transformers.\n",
    "* The last step can be a regressor, a classifier, a clusterer, a transformer, etc. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "from river import linear_model\n",
    "from river import preprocessing\n",
    "from river import feature_extraction\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model can be visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `compose.Pipeline` inherits from `base.Estimator` and thus has a `learn_one` method.\n",
    "* Before river v.0.19.0, the `learn_one` method updated the supervised parts of the pipeline.\n",
    "* This behavior change in river v.0.19.0, where the `learn_one` method now updates the entire pipeline.\n",
    "* The `predict_one` method updated the unsupervised parts of the pipeline (and not the supervised parts?).\n",
    "* Let's see how this works in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "\n",
    "dataset = datasets.TrumpApproval()\n",
    "x, y = next(iter(dataset))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Take a look at the unsupervised parts of the pipeline, i.e. the `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['StandardScaler'].means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The unsupervised parts of the pipeline empty, because no data has been passed through the pipeline yet.\n",
    "* If we call the `predict_one` method, only the unsupervised parts of the pipeline are updated, but not the model (supervised parts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_one(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['StandardScaler'].means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see, the means of each feature have been updated (they were standardized and have a zero mean), even though we called `predict_one` and not `learn_one`.\n",
    "* Now let's take a look at the supervised parts of the pipeline, i.e., the model and its weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"LinearRegression\"].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since we have called `predict_one` and not `learn_one`, the model has not been updated.\n",
    "* The model weights are empty, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can even call `transform_one` with this model (that does not have any weights) and it will work, because the output from the last transformer (which is thus the penultimate step) will be returned in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform_one(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calling `transform_one` does not update the model weights, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"LinearRegression\"].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, usually we want to update the model weights, so we should call `learn_one`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn_one(x, y)\n",
    "model[\"LinearRegression\"].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, after calling `learn_one`, the model weights have been updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "\n",
    "dataset = datasets.Bananas()\n",
    "x, y = next(iter(dataset))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model\n",
    "from river import optim\n",
    "from river import preprocessing\n",
    "from river import stream\n",
    "from river import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "optimizer = optim.SGD(lr=0.01)\n",
    "log_reg = linear_model.LogisticRegression(optimizer)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for xi, yi in dataset:\n",
    "\n",
    "    # Scale the features\n",
    "    scaler.learn_one(xi)\n",
    "    xi_scaled = scaler.transform_one(xi)\n",
    "\n",
    "    # Test the current model on the new \"unobserved\" sample\n",
    "    yi_pred = log_reg.predict_proba_one(xi_scaled)\n",
    "    # Train the model with the new sample\n",
    "    log_reg.learn_one(xi_scaled, yi)\n",
    "\n",
    "    # Store the truth and the prediction\n",
    "    y_true.append(yi)\n",
    "    y_pred.append(yi_pred[True])\n",
    "\n",
    "print(f\"y: {y_true[:5]}\")\n",
    "print(f\"y_pred: {y_pred[:5]}\")\n",
    "# print(f'ROC AUC: {metrics.Accuracy(y_true, y_pred):.3f}')\n",
    "print(f'ROC AUC: {roc_auc_score(y_true, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric and metric.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "from river import datasets\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "model = linear_model.LogisticRegression()\n",
    "metric = metrics.ROCAUC()\n",
    "for x, y in dataset:\n",
    "    y_pred = model.predict_proba_one(x)\n",
    "    model.learn_one(x, y)\n",
    "    print(f\"y: {y}, y_pred: {y_pred}\")\n",
    "    metric.update(y, y_pred)\n",
    "\n",
    "metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-then-train \n",
    "\n",
    "* Now the single samples of the dataset are used for testing and training. \n",
    "  * For each sample, the metric is updated.\n",
    "* Finally, the metric that was incrementally calculated on the data from the entire process is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a classification dataset with sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=1_000, n_features=3, n_informative=2, n_redundant=1, random_state=42)\n",
    "\n",
    "## Convert X to a pandas dataframe\n",
    "import pandas as pd\n",
    "X = pd.DataFrame(X)\n",
    "# convert y to boolean\n",
    "y = pd.Series(y).astype(bool)\n",
    "y = pd.Series(y)\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream\n",
    "from river import linear_model\n",
    "from river import preprocessing\n",
    "from river import metrics\n",
    "\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "metric_list = []\n",
    "\n",
    "for xi, yi in stream.iter_pandas(X, y):\n",
    "    \n",
    "    print(f\"xi: {xi}\")\n",
    "    \n",
    "    # Before river 0.21.0:\n",
    "    # Scale the features\n",
    "    #xi_scaled = scaler.learn_one(xi).transform_one(xi)\n",
    "\n",
    "    # After 0.21.0:\n",
    "    scaler.learn_one(xi)\n",
    "    xi_scaled = scaler.transform_one(xi)\n",
    "    \n",
    "    print(f\"xi_scaled: {xi_scaled}\")\n",
    "    \n",
    "    # Test the current model on the new \"unobserved\" sample\n",
    "    yi_pred = log_reg.predict_proba_one(xi_scaled)\n",
    "    # yi_pred = log_reg.predict_one(xi_scaled)\n",
    "    print(f\"yi: {yi}, yi_pred: {yi_pred}\")\n",
    "\n",
    "    # Train the model with the new sample\n",
    "    log_reg.learn_one(xi_scaled, yi)\n",
    "    \n",
    "    metric.update(yi, yi_pred)\n",
    "    print(f\"metric: {metric}\")\n",
    "    # Store the metric after each sample in a list\n",
    "    metric_list.append(metric.get())\n",
    "    \n",
    "    # Store the truth and the prediction\n",
    "    y_true.append(yi)\n",
    "    y_pred.append(yi_pred)\n",
    "    print(f\"y: {y_true[:5]}\")\n",
    "    print(f\"y_pred: {y_pred[:5]}\")   \n",
    "    \n",
    "\n",
    "print(f\"Final metric: {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the values of the metric_list versus the sample number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the values of the metric_list verus the number of samples\n",
    "plt.plot(metric_list)\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Da der `SGD` nicht-deterministisch ist, führt jeder Aufruf zu einem (leicht) modifizierten Ergebnis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (ALMA) Classification for Synthetic Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data \n",
    "\n",
    "* Synthetic classification data is generated using the `make_classification` function from the `sklearn` package, see [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html).\n",
    "  * This function allows the creation of classification problems with $n$ classes.\n",
    "  * In our example, $n=2$ is chosen (this is the default).  \n",
    "* The data is then split into the training and test datasets using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from river import linear_model\n",
    "\n",
    "X, y = make_classification(shuffle=True, n_samples=2000)\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.ALMAClassifier()\n",
    "\n",
    "# fit the model\n",
    "for x_i,y_i in zip(X_train,y_train):\n",
    "    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}\n",
    "    model.learn_one(x_json,y_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the test set and compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for x_i in X_test:\n",
    "    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}\n",
    "    preds.append(model.predict_one(x_json))\n",
    "\n",
    "# compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the scikit-learn \"classification report\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (ALMA) classification for a sklearn classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a classification dataset with sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=2, n_redundant=2, random_state=42)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "\n",
    "## Convert X to a pandas dataframe\n",
    "import pandas as pd\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model is fitted on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.ALMAClassifier()\n",
    "for xi, yi in stream.iter_pandas(X_train,y_train):\n",
    "\n",
    "    # Before river 0.21.0:\n",
    "    # Scale the features\n",
    "    # xi_scaled = scaler.learn_one(xi).transform_one(xi)\n",
    "\n",
    "    # After 0.21.0:\n",
    "    scaler.learn_one(xi)\n",
    "    xi_scaled = scaler.transform_one(xi)\n",
    "\n",
    "    model.learn_one(xi_scaled, yi) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Prediction on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for xi, _ in stream.iter_pandas(X_test):    \n",
    "\n",
    "    # Before river 0.21.0:\n",
    "    # Scale the features\n",
    "    # xi_scaled = scaler.learn_one(xi).transform_one(xi)\n",
    "\n",
    "    # After 0.21.0:\n",
    "    scaler.learn_one(xi)\n",
    "    xi_scaled = scaler.transform_one(xi)\n",
    "\n",
    "    preds.append(model.predict_one(xi_scaled))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Computation of the accuracy metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passiv-Aggressiv Classification for Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X,y=make_classification(shuffle=True,n_samples=2000)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "from river import linear_model\n",
    "\n",
    "model = linear_model.PAClassifier()\n",
    "\n",
    "for x_i,y_i in zip(X_train,y_train):\n",
    "    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}\n",
    "    model.learn_one(x_json,y_i)\n",
    "    \n",
    "\n",
    "preds = []\n",
    "for x_i in X_test:\n",
    "    x_json = {'val'+str(i): x for i,x in enumerate(x_i)}\n",
    "    preds.append(model.predict_one(x_json))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "from river import datasets\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "from river.datasets import synth\n",
    "from river import tree\n",
    "\n",
    "from river.datasets import synth\n",
    "\n",
    "dataset = synth.SEA(variant=0, seed=42)\n",
    "#dataset = datasets.Phishing()\n",
    "data = dataset.take(10000)\n",
    "\n",
    "model = tree.HoeffdingTreeClassifier(grace_period=50)\n",
    "\n",
    "for x, y in data:\n",
    "    print(x, y)\n",
    "    x = {f'x_{key}': value for key, value in x.items()}\n",
    "    print(x,y)\n",
    "    model.learn_one(x, y)\n",
    "\n",
    "model\n",
    "\n",
    "model.summary\n",
    "\n",
    "model.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('spotCondaEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81c77de872def749acd68d9955e19f0df6803301f4c1f66c3444af66334112ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
