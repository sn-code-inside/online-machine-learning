{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "title: '`river` Hyperparameter Tuning: SGTRegressor with Friedman Drift Data'\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook is part of the supplementary material of the books \"Online Machine Learning - Eine praxisorientiere Einf√ºhrung\",  \n",
        "https://link.springer.com/book/9783658425043 and \"Online Machine Learning - A Practical Guide with Examples in Python\" https://link.springer.com/book/9789819970063\n",
        "The contents are open source and published under the \"BSD 3-Clause License\".\n",
        "This software is provided \"as is\" without warranty of any kind, either express or implied, including but not limited to implied warranties of merchantability and fitness for a particular purpose. The author or authors assume no liability for any damages or liability, whether in contract, tort, or otherwise, arising out of or in connection with the software or the use or other dealings with the software."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import inf\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from spotRiver.utils.data_conversion import convert_to_df\n",
        "from river.datasets import synth\n",
        "import warnings\n",
        "if not os.path.exists('./figures'):\n",
        "    os.makedirs('./figures')\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "PREFIX=\"CH10_SGT\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* This notebook exemplifies hyperparameter tuning with SPOT (spotPython and spotRiver).\n",
        "* This notebook demonstrates hyperparameter tuning for `river`. It is based on the notebook \"Incremental decision trees in river: the Hoeffding Tree case\", see: [https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/#42-regression-tree-splitters](https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/#42-regression-tree-splitters).\n",
        "* Here we will use the river `HTR` and `HATR` functions as in \"Incremental decision trees in river: the Hoeffding Tree case\", see: [https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/#42-regression-tree-splitters](https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/#42-regression-tree-splitters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Friedman Drift Data Set {#sec-the-friedman-drift-data-set-24}\n",
        "\n",
        "We will use the Friedman synthetic dataset with concept drifts, which is described in detail in @sec-a-05-friedman.\n",
        "The following parameters are used to generate and handle the data set:\n",
        "\n",
        "* `position`: The positions of the concept drifts.\n",
        "* `n_train`: The number of samples used for training.\n",
        "* `n_val`: The number of samples used for validation. This data set is used during the hyperparameter tuning as a validation set.\n",
        "* `n_test`: The number of samples used for testing. This data set is not used during the hyperparameter tuning.\n",
        "* `seed`: The seed for the random number generator.\n",
        "* `target_column`: The name of the target column.\n",
        "* `drift_type`: The type of the concept drift.\n",
        "\n",
        "We will use `spotRiver`'s `convert_to_df` function [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/utils/data_conversion.py) to convert the `river` data set to a `pandas` data frame.\n",
        "Then we add column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column of the dataframe.\n",
        "\n",
        "This data generation is later independently repeated for the training and test data sets to obtain different data sets for training and testing (better validation) during the tuning and the final evaluation after the tuning is finished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We generate a small training data set with 7*42 samples and a large validation data set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from river.datasets import synth\n",
        "from spotRiver.utils.data_conversion import convert_to_df\n",
        "\n",
        "target_column = \"y\"\n",
        "metric = mean_absolute_error\n",
        "horizon = 7*24\n",
        "n_train = horizon\n",
        "p_1 = int(n_train/4)\n",
        "p_2 = int(n_train/2)\n",
        "position=(p_1, p_2)\n",
        "dataset_train = synth.FriedmanDrift(\n",
        "   drift_type='gra',\n",
        "   position=position,\n",
        "   seed=123\n",
        ")\n",
        "\n",
        "train = convert_to_df(dataset_train, n_total=n_train)\n",
        "train.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_val = 10_000\n",
        "p_1 = int(n_val/4)\n",
        "p_2 = int(n_val/2)\n",
        "position=(p_1, p_2)\n",
        "dataset_val = synth.FriedmanDrift(\n",
        "   drift_type='gra',\n",
        "   position=position,\n",
        "   seed=124\n",
        ")\n",
        "val = convert_to_df(dataset_val, n_total=n_val)\n",
        "val.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data sets that are available as `pandas` dataframes can easily be passed to the `spot` hyperparameter tuner.\n",
        "`spotpython` requires a `train` and a `test` data set, where the column names must be identical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# combine the train and val data (train data first, val data second) in order to have a single dataset and save it to a csv file. Include the number of train and val samples in the file name.\n",
        "data = pd.concat([train, val])\n",
        "data.to_csv(f\"./data/synth_friedman_drift_train{n_train}_val{n_val}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "### General Experiment Setup\n",
        "\n",
        "To keep track of the different experiments, we use a `PREFIX` for the experiment name. The `PREFIX` is used to create a unique experiment name. The `PREFIX` is also used to create a unique TensorBoard folder, which is used to store the TensorBoard log files.\n",
        "\n",
        "`spotpython` allows the specification of two different types of stopping criteria: first, the number of function evaluations (`fun_evals`), and second, the maximum run time in seconds (`max_time`). Here, we will set the number of function evaluations to infinity and the maximum run time to sixty minutes.\n",
        "\n",
        "Furthermore, we set the initial design size (`init_size`) to 10. The initial design is used to train the surrogate model. The surrogate model is used to predict the performance of the hyperparameter configurations. The initial design is also used to train the first model. Since the `init_size` belongs to the experimental design, it is set in the `design_control` dictionary, see [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/utils/init/#spotPython.utils.init.design_control_init).\n",
        "\n",
        "`max_time` is set to one minute for demonstration purposes and `init_size` is set to 10 for demonstration purposes. For real experiments, these values should be increased.\n",
        "Note,  the total run time may exceed the specified `max_time`, because the initial design is always evaluated, even if this takes longer than `max_time`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Setup and Preprocessing\n",
        "\n",
        "We use the `StandardScaler` [[SOURCE]](https://riverml.xyz/dev/api/preprocessing/StandardScaler/) from `river` as the data-preprocessing model. The `StandardScaler` is used to standardize the data set, i.e., it has zero mean and unit variance.\n",
        "\n",
        "The names of the training and test data sets are `train` and `test`, respectively. They are available as `pandas` dataframes.\n",
        "Both must use the same column names. The column names were set to `x1` to `x10` for the features and `y` for the target column during the data set generation in @sec-the-friedman-drift-data-set-24. Therefore, the `target_column` is set to `y` (as above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Setup\n",
        "\n",
        "Here we use the `mean_absolute_error` [[SOURCE]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) as the evaluation metric.\n",
        "Internally, this metric is passed to the objective (or loss) function `fun_oml_horizon` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/fun/hyperriver.py) and further to the evaluation function `eval_oml_horizon` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_bml.py).\n",
        "\n",
        "`spotRiver` also supports additional metrics. For example, the `metric_river` is used for the river based evaluation via `eval_oml_iter_progressive` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/evaluation/eval_oml.py). The `metric_river` is implemented to simulate the behaviour of the \"original\" `river` metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### River-Specific Setup \n",
        "\n",
        "In the online-machine-learning (OML) setup, the model is trained on a fixed number of observations and then evaluated on a fixed number of observations. The `horizon` defines the number of observations that are used for the evaluation. Here, a horizon of 7*24 is used, which corresponds to one week of data.\n",
        "\n",
        "The `oml_grace_period` defines the number of observations that are used for the initial training of the model.\n",
        "This value is relatively small, since the online-machine-learning is trained on the incoming data and the model is updated continuously.\n",
        "However, it needs a certain number of observations to start the training process.\n",
        "Therefore, this short training period aka `oml_grace_period` is set to the horizon, i.e., the number of observations that are used for the evaluation.\n",
        "In this case, we use a horizon of 7*24.\n",
        "\n",
        "The `weights` provide a flexible way to define specific requirements, e.g., if the memory is more important than the time, the weight for the memory can be increased. `spotRiver` stores information about the model' s score (metric), memory, and time. The hyperparamter tuner requires a single objective.\n",
        "Therefore, a weighted sum of the metric, memory, and time is computed. The weights are defined in the `weights` array.\n",
        "The `weights` provide a flexible way to define specific requirements, e.g., if the memory is more important than the time, the weight for the memory can be increased.\n",
        "\n",
        "The `weight_coeff` defines a multiplier for the results: results are multiplied by (step/n_steps)**weight_coeff, where n_steps is the total number of iterations.\n",
        "Results from the beginning have a lower weight than results from the end if weight_coeff > 1. If weight_coeff == 0, all results have equal weight. Note, that the `weight_coeff` is only used internally for the tuner and does not affect the results that are used for the evaluation or comparisons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Setup\n",
        "\n",
        "By using `core_model_name = \"tree.HoeffdingTreeRegressor\"`, the `river` model class `HoeffdingTreeRegressor` [[SOURCE]](https://riverml.xyz/dev/api/tree/HoeffdingTreeRegressor/) from the `tree` module is selected.\n",
        "For a given `core_model_name`, the corresponding hyperparameters are automatically loaded from the associated dictionary, which is stored as a JSON file. The JSON file contains hyperparameter type information, names, and bounds. For `river` models, the hyperparameters are stored in the `RiverHyperDict`, see [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json)\n",
        "\n",
        "Alternatively, you can load a local hyper_dict. Simply set `river_hyper_dict.json` as the filename. If `filename`is set to `None`, which is the default, the hyper_dict [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/data/river_hyper_dict.json) is loaded from the `spotRiver` package.\n",
        "\n",
        "How hyperparameter levels can be modified is described in @sec-modifying-hyperparameter-levels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Objective Function Setup\n",
        "\n",
        "The loss function (metric) values are passed to the objective function `fun_oml_horizon` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotRiver/blob/main/src/spotRiver/fun/hyperriver.py), which combines information about the loss, required memory and time as described in @sec-river-specific-setup-24."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Surrogate Model Setup\n",
        "\n",
        "The default surrogate model is the `Kriging` model, see [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/build/kriging/). We specify `noise` as `True` to include noise in the model. An `anisotropic` kernel is used, which allows different length scales for each dimension, by setting `n_theta = 2`. Furthermore, the interval for the `Lambda` value is set to `[1e-3, 1e2]`.\n",
        "\n",
        "These parameters are set in the `surrogate_control` dictionary and therefore passed  to the `surrogate_control_init` function [[SOURCE]](https://sequential-parameter-optimization.github.io/spotPython/reference/spotPython/utils/init/#spotPython.utils.init.surrogate_control_init)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary: Setting up the Experiment\n",
        "\n",
        "At this stage, all required information is available to set up the dictionaries for the hyperparameter tuning.\n",
        "Altogether, the `fun_control`, `design_control`, `surrogate_control`, and `optimize_control` dictionaries are initialized as follows (the TENSORBOARD-related arguments are explained in the section \"TensorBoard\" below.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import inf\n",
        "import numpy as np\n",
        "from spotRiver.fun.hyperriver import HyperRiver\n",
        "from spotRiver.hyperdict.river_hyper_dict import RiverHyperDict\n",
        "from spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
        "\n",
        "fun = HyperRiver().fun_oml_horizon\n",
        "\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=PREFIX,\n",
        "    TENSORBOARD_CLEAN=False,\n",
        "    tensorboard_start=False,\n",
        "    tensorboard_stop=False,\n",
        "    fun_evals=inf,\n",
        "    max_time=60,\n",
        "\n",
        "    prep_model_name=\"StandardScaler\",\n",
        "    test=val, # tuner uses the validation set as test set\n",
        "    train=train,\n",
        "    target_column=target_column,\n",
        "\n",
        "    metric_sklearn_name=\"mean_absolute_error\",\n",
        "    horizon=7*24,\n",
        "    oml_grace_period=7*24,\n",
        "    weight_coeff=0.0,\n",
        "    weights=np.array([100, 0.1, 0.1]),\n",
        "\n",
        "    core_model_name=\"tree.SGTRegressor\",\n",
        "    hyperdict=RiverHyperDict,\n",
        "   )\n",
        "\n",
        "\n",
        "design_control = design_control_init(\n",
        "    init_size=20,\n",
        ")\n",
        "\n",
        "surrogate_control = surrogate_control_init(\n",
        "    noise=True,\n",
        "    n_theta=2,\n",
        "    min_Lambda=0.001,\n",
        "    max_Lambda=100,\n",
        ")\n",
        "\n",
        "optimizer_control = optimizer_control_init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modify `hyper_dict` Hyperparameters for the Selected Algorithm aka `core_model`\n",
        "\n",
        "* Numeric, boolean values and factors can be modified using the set_hyperparameter method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparameters can be excluded from the tuning procedure by selecting identical values for the lower and upper bounds. For example, the hyperparameter `merit_preprune` is excluded from the tuning procedure by setting the bounds to `[0, 0]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set_hyperparameter(fun_control,  \"merit_preprune\", [0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the `Spot` Optimizer\n",
        "\n",
        "The class `Spot` [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/spot/spot.py) is the hyperparameter tuning workhorse. It is initialized with the following parameters, which were specified above.\n",
        "\n",
        "* `fun`: the objective function\n",
        "* `fun_control`: the dictionary with the control parameters for the objective function\n",
        "* `design_control`: the dictionary with the control parameters for the experimental design\n",
        "* `surrogate_control`: the dictionary with the control parameters for the surrogate model\n",
        "* `optimizer_control`: the dictionary with the control parameters for the optimizer\n",
        "\n",
        "`spotpython` allows maximum flexibility in the definition of the hyperparameter tuning setup. Alternative surrogate models, optimizers, and experimental designs can be used. Thus, interfaces for the `surrogate` model, experimental `design`, and `optimizer` are provided. The default surrogate model is the kriging model, the default optimizer is the differential evolution, and default experimental design is the Latin hypercube design."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.spot import spot\n",
        "spot_tuner = spot.Spot(\n",
        "    fun=fun,\n",
        "    fun_control=fun_control,\n",
        "    design_control=design_control,\n",
        "    surrogate_control=surrogate_control,\n",
        "    optimizer_control=optimizer_control,\n",
        ")\n",
        "res = spot_tuner.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "`spotPython`'s method `gen_design_table` summarizes the experimental design that is used for the hyperparameter tuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.eda import gen_design_table\n",
        "print(gen_design_table(fun_control))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorBoard {#sec-tensorboard-13}\n",
        "\n",
        "* In order to use Tensorbord, the `tensorboard_start` argument of the `fun_control_init()` function must be set to `True`.\n",
        "* The `tensorboard_stop` argument of the `fun_control_init()` function cant be set to `True` to stop the TensorBoard process after the hyperparameter tuning is finished.\n",
        "* The TensorBoard log files are stored in the `./runs` directory. After several runs, this directory may contain a large number of log files. Therefore, it is recommended to delete the log files before starting a new hyperparameter tuning run. Setting the `TENSORBOARD_CLEAN` argument to `True` in the `fun_control_init()` function will automatically move existing log files from previous runs to the folder `runs_old`, so that the hyperparameter tuning run is started in a clean `run` directory.\n",
        "* TensorBoard can be started in the background with the following command, where `./runs` is the default directory for the TensorBoard log files:\n",
        "\n",
        "`tensorboard --logdir=\"./runs\" --port=6006 &`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-tip}\n",
        "#### Tip: TENSORBOARD_PATH\n",
        "The TensorBoard path can be printed with the following command:\n",
        "\n",
        "`from spotPython.utils.file import get_tensorboard_path`\n",
        "`get_tensorboard_path(fun_control)`\n",
        "\n",
        ":::\n",
        "\n",
        "We can access the TensorBoard web server with the following URL:\n",
        "\n",
        "`http://localhost:6006/`\n",
        "\n",
        "The TensorBoard plot illustrates how `spotPython` can be used as a microscope for the internal mechanisms of the surrogate-based optimization process. Here, one important parameter, the learning rate $\\theta$ of the Kriging surrogate [[SOURCE]](https://github.com/sequential-parameter-optimization/spotPython/blob/main/src/spotPython/build/kriging.py) is plotted against the number of optimization steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized with `spotpython`'s method `plot_progress`. The black points represent the performace values (score or metric) of  hyperparameter configurations from the initial design, whereas the red points represents the  hyperparameter configurations found by the surrogate model based optimization.After the hyperparameter tuning run is finished, the results can be saved and reloaded with the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_progress(log_y=True, filename=\"./figures/\" + PREFIX +\"_progress.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results can also be printed in tabular form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.eda import gen_design_table\n",
        "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A histogram can be used to visualize the most important hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_importance(threshold=1, filename=\"./figures/\" + PREFIX+\"_importance.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Default Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The default hyperparameters, which will be used for a comparison with the tuned hyperparameters, can be obtained with the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperparameters.values import get_one_core_model_from_X\n",
        "from spotPython.hyperparameters.values import get_default_hyperparameters_as_array\n",
        "X_start = get_default_hyperparameters_as_array(fun_control)\n",
        "model_default = get_one_core_model_from_X(X_start, fun_control)\n",
        "model_default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Geneate train and test data as above, but with different seeds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_column = \"y\"\n",
        "metric = mean_absolute_error\n",
        "horizon = 7*24\n",
        "n_train = horizon\n",
        "p_1 = int(n_train/4)\n",
        "p_2 = int(n_train/2)\n",
        "position=(p_1, p_2)\n",
        "dataset_train = synth.FriedmanDrift(\n",
        "   drift_type='gra',\n",
        "   position=position,\n",
        "   seed=125\n",
        ")\n",
        "\n",
        "train = convert_to_df(dataset_train, n_total=n_train)\n",
        "train.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_test = 1_000_000\n",
        "p_1 = int(n_test/4)\n",
        "p_2 = int(n_test/2)\n",
        "position=(p_1, p_2)\n",
        "dataset_test = synth.FriedmanDrift(\n",
        "   drift_type='gra',\n",
        "   position=position,\n",
        "   seed=126\n",
        ")\n",
        "test = convert_to_df(dataset_test, n_total=n_test)\n",
        "test.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
        "\n",
        "df_eval_default, df_true_default = eval_oml_horizon(\n",
        "                    model=model_default,\n",
        "                    train=train,\n",
        "                    test=test,\n",
        "                    target_column=fun_control[\"target_column\"],\n",
        "                    horizon=fun_control[\"horizon\"],\n",
        "                    oml_grace_period=fun_control[\"oml_grace_period\"],\n",
        "                    metric=fun_control[\"metric_sklearn\"],\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The three performance criteria, i.e., score (metric), runtime, and memory consumption, can be visualized with the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotRiver.evaluation.eval_bml import plot_bml_oml_horizon_metrics, plot_bml_oml_horizon_predictions\n",
        "df_labels=[\"default\"]\n",
        "plot_bml_oml_horizon_metrics(df_eval = [df_eval_default], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"], skip_first_n=10, skip_last_n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Select a subset of the data set for the visualization of the predictions:\n",
        "    * We use the mean, $m$, of the data set as the center of the visualization.\n",
        "    * We use 100 data points, i.e., $m \\pm 50$ as the visualization window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = fun_control[\"test\"].shape[0]\n",
        "a = int(m/2)-50\n",
        "b = int(m/2)\n",
        "plot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b]], target_column=target_column,  df_labels=df_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get SPOT Results\n",
        "\n",
        "In a similar way, we can obtain the hyperparameters found by `spotPython`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperparameters.values import get_one_core_model_from_X\n",
        "X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n",
        "model_spot = get_one_core_model_from_X(X, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_eval_spot, df_true_spot = eval_oml_horizon(\n",
        "                    model=model_spot,\n",
        "                    train=train,\n",
        "                    test=test,\n",
        "                    target_column=fun_control[\"target_column\"],\n",
        "                    horizon=fun_control[\"horizon\"],\n",
        "                    oml_grace_period=fun_control[\"oml_grace_period\"],\n",
        "                    metric=fun_control[\"metric_sklearn\"],\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_labels=[\"default\", \"spot\"]\n",
        "plot_bml_oml_horizon_metrics(df_eval = [df_eval_default, df_eval_spot], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"], filename=\"./figures/\" +PREFIX+\"_metrics.pdf\", skip_first_n=10, skip_last_n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b], df_true_spot[a:b]], target_column=target_column,  df_labels=df_labels, filename=\"./figures/\" + PREFIX +\"_predictions.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.plot.validation import plot_actual_vs_predicted\n",
        "plot_actual_vs_predicted(y_test=df_true_default[target_column], y_pred=df_true_default[\"Prediction\"], title=\"Default\")\n",
        "plot_actual_vs_predicted(y_test=df_true_spot[target_column], y_pred=df_true_spot[\"Prediction\"], title=\"SPOT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Regression Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_f = dataset_test.take(n_test)\n",
        "for x, y in dataset_f:\n",
        "    model_default.learn_one(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-caution}\n",
        "### Caution: Large Trees\n",
        "* Since the trees are large, the visualization is suppressed by default.\n",
        "* To visualize the trees, uncomment the following line.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_default.draw()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_default.summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spot Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_f = dataset_test.take(n_test)\n",
        "for x, y in dataset_f:\n",
        "    model_spot.learn_one(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-caution}\n",
        "### Caution: Large Trees\n",
        "* Since the trees are large, the visualization is suppressed by default.\n",
        "* To visualize the trees, uncomment the following line.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_spot.draw()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_spot.summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.eda import compare_two_tree_models\n",
        "print(compare_two_tree_models(model_default, model_spot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Hyperparameter Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = \"./figures/\" + PREFIX + \"_important_hyperparameter_contour.pdf\"\n",
        "spot_tuner.plot_important_hyperparameter_contour(filename=filename, max_imp=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallel Coordinates Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.parallel_plot()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
